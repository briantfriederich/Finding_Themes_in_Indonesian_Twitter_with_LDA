{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Themes in Indonesian Twitter with LSA and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           isi_tweet  sentimen\n",
      "0  tidak setuju jokowi jadi cawapres capres jokow...         1\n",
      "1  capres jokowi wacapres abraham samad gubernur ...         1\n",
      "2  capres prabowo dan cawapres jokowi dan gubdki ...         1\n",
      "3  jadi skenarionya gini 2014 biar prabowo jadi p...         1\n",
      "4  sby mantan tni dan calon presiden prabowo subi...         1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tweets.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           isi_tweet  index\n",
      "0  tidak setuju jokowi jadi cawapres capres jokow...      0\n",
      "1  capres jokowi wacapres abraham samad gubernur ...      1\n",
      "2  capres prabowo dan cawapres jokowi dan gubdki ...      2\n",
      "3  jadi skenarionya gini 2014 biar prabowo jadi p...      3\n",
      "4  sby mantan tni dan calon presiden prabowo subi...      4\n",
      "\n",
      "1846 tweets in dataset\n"
     ]
    }
   ],
   "source": [
    "data_text = data[['isi_tweet']]\n",
    "data_text['index'] = data_text.index\n",
    "tweets = data_text\n",
    "print(tweets.head())\n",
    "print(\"\\n{} tweets in dataset\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianfrieerich/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "np.random.seed(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Original: Mereka meniru-nirukannya\n",
      "\n",
      "Stemmed: mereka tiru\n",
      "-----\n",
      "Original: Saya pembantu\n",
      "\n",
      "Stemmed: saya bantu\n",
      "-----\n",
      "Original: Dia dipanggil oleh wanita tercantik\n",
      "\n",
      "Stemmed: dia panggil oleh wanita cantik\n"
     ]
    }
   ],
   "source": [
    "factory = StemmerFactory()\n",
    "indoStemmer = factory.create_stemmer()\n",
    "\n",
    "def indoStem(text):\n",
    "    stemmed = indoStemmer.stem(text)\n",
    "    return stemmed\n",
    "\n",
    "sentences = [\"Mereka meniru-nirukannya\", \"Saya pembantu\", \"Dia dipanggil oleh wanita tercantik\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"-----\\nOriginal: {}\".format(sentence))\n",
    "    output = indoStem(sentence)\n",
    "    print(\"\\nStemmed: {}\".format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = []\n",
    "with open('stopwords.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    stopwords_list = list(reader) \n",
    "    flat_stoplist = [item for sublist in stopwords_list for item in sublist]\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    stemmed = indoStem(text)\n",
    "    for word in stemmed.split(' '):\n",
    "        if word not in flat_stoplist:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tuju', 'jokowi', 'cawapres', 'capres', 'jokowi', 'harga', 'mati'],\n",
       " ['capres',\n",
       "  'jokowi',\n",
       "  'wacapres',\n",
       "  'abraham',\n",
       "  'samad',\n",
       "  'gubernur',\n",
       "  'ahok',\n",
       "  'koruptor',\n",
       "  'abissss'],\n",
       " ['capres',\n",
       "  'prabowo',\n",
       "  'cawapres',\n",
       "  'jokowi',\n",
       "  'gubdki',\n",
       "  'ahok',\n",
       "  'mantap',\n",
       "  'presiden',\n",
       "  'sby',\n",
       "  'bubar',\n",
       "  'fpi'],\n",
       " ['skenario',\n",
       "  'gin',\n",
       "  'biar',\n",
       "  'prabowo',\n",
       "  'presiden',\n",
       "  'jokowi',\n",
       "  'tetepgubernur',\n",
       "  'jakarta',\n",
       "  'hasil',\n",
       "  'nunggu',\n",
       "  'gantiin',\n",
       "  'prabowo'],\n",
       " ['sby',\n",
       "  'mantan',\n",
       "  'tni',\n",
       "  'calon',\n",
       "  'presiden',\n",
       "  'prabowo',\n",
       "  'subianto',\n",
       "  'mantan',\n",
       "  'kopassus',\n",
       "  'anggoto',\n",
       "  'tni',\n",
       "  'disiplin',\n",
       "  'smw',\n",
       "  'presiden']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets = list(map(preprocess, tweets['isi_tweet']))\n",
    "processed_tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below = 15, no_above = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (3, 1), (20, 2), (22, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(twt) for twt in processed_tweets]\n",
    "bow_corpus[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"ahok\") appears 1 time.\n",
      "Word 3 (\"gubernur\") appears 1 time.\n",
      "Word 20 (\"ya\") appears 2 time.\n",
      "Word 22 (\"indonesia\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_tweet_14 = bow_corpus[15]\n",
    "for i in range(len(bow_tweet_14)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_tweet_14[i][0], \n",
    "                                                     dictionary[bow_tweet_14[i][0]], \n",
    "                                                     bow_tweet_14[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5104769343386324), (1, 0.8598914463513587)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.381*\"jk\" + 0.374*\"hatta\" + 0.259*\"gubernur\" + 0.225*\"nyata\" + 0.217*\"tokoh\" + 0.212*\"cawapres\" + 0.203*\"sosok\" + 0.200*\"populer\" + 0.200*\"bincang\" + 0.197*\"ketua\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: -0.420*\"hatta\" + 0.319*\"gubernur\" + 0.297*\"nyata\" + 0.286*\"sosok\" + 0.283*\"populer\" + 0.282*\"bincang\" + 0.282*\"tokoh\" + -0.214*\"buka\" + -0.211*\"pan\" + -0.207*\"ketua\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: -0.644*\"jk\" + 0.227*\"hatta\" + -0.215*\"dahlan\" + -0.205*\"arb\" + 0.188*\"sosok\" + 0.187*\"gubernur\" + 0.186*\"populer\" + 0.185*\"bincang\" + 0.169*\"tokoh\" + 0.168*\"nyata\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsa_model = gensim.models.LsiModel(corpus_tfidf, \n",
    "                                   num_topics = num_topics, \n",
    "                                   id2word=dictionary)\n",
    "for idx, topic in lsa_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.078*\"dahlan\" + 0.061*\"arb\" + 0.046*\"pilih\" + 0.038*\"iskan\" + 0.035*\"rakyat\" + 0.030*\"wiranto\" + 0.028*\"dukung\" + 0.027*\"jk\" + 0.024*\"ya\" + 0.020*\"konvensi\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.071*\"jk\" + 0.045*\"gubernur\" + 0.044*\"calon\" + 0.031*\"tokoh\" + 0.031*\"nyata\" + 0.030*\"mahfud\" + 0.028*\"indonesia\" + 0.027*\"pdip\" + 0.026*\"sosok\" + 0.025*\"populer\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.112*\"hatta\" + 0.052*\"ketua\" + 0.051*\"cawapres\" + 0.044*\"pan\" + 0.044*\"buka\" + 0.039*\"evaluasi\" + 0.039*\"pencapresan\" + 0.038*\"radjasa\" + 0.026*\"survey\" + 0.020*\"tweet\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics = num_topics, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers = 4)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.068*\"jk\" + 0.041*\"dukung\" + 0.038*\"wiranto\" + 0.032*\"mahfud\" + 0.032*\"pdip\" + 0.026*\"mega\" + 0.022*\"md\" + 0.022*\"kalah\" + 0.020*\"ical\" + 0.020*\"menang\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.046*\"hatta\" + 0.038*\"calon\" + 0.037*\"indonesia\" + 0.033*\"pilih\" + 0.026*\"wapres\" + 0.026*\"jakarta\" + 0.025*\"ketua\" + 0.023*\"ya\" + 0.022*\"buka\" + 0.021*\"tweet\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.051*\"dahlan\" + 0.043*\"arb\" + 0.033*\"rakyat\" + 0.030*\"nyata\" + 0.029*\"iskan\" + 0.028*\"gubernur\" + 0.026*\"sby\" + 0.026*\"sosok\" + 0.025*\"tokoh\" + 0.025*\"maju\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics = num_topics, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)\n",
    "                                             \n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prabowo subianto vs joko widodo no prabowo subianto presiden jokowi wapres yes aminin yaaaaa allah\n",
      "['prabowo', 'subianto', 'vs', 'joko', 'widodo', 'no', 'prabowo', 'subianto', 'presiden', 'jokowi', 'wapres', 'yes', 'aminin', 'yaaaaa', 'allah']\n"
     ]
    }
   ],
   "source": [
    "document_num = 13\n",
    "print(data.iloc[document_num, 0])\n",
    "\n",
    "print(processed_tweets[document_num])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.037299727710397364\t \n",
      "Topic: 0.381*\"jk\" + 0.374*\"hatta\" + 0.259*\"gubernur\" + 0.225*\"nyata\" + 0.217*\"tokoh\" + 0.212*\"cawapres\" + 0.203*\"sosok\" + 0.200*\"populer\" + 0.200*\"bincang\" + 0.197*\"ketua\"\n",
      "\n",
      "Score: -0.0010628898567282214\t \n",
      "Topic: -0.420*\"hatta\" + 0.319*\"gubernur\" + 0.297*\"nyata\" + 0.286*\"sosok\" + 0.283*\"populer\" + 0.282*\"bincang\" + 0.282*\"tokoh\" + -0.214*\"buka\" + -0.211*\"pan\" + -0.207*\"ketua\"\n",
      "\n",
      "Score: -0.03145815454014468\t \n",
      "Topic: -0.644*\"jk\" + 0.227*\"hatta\" + -0.215*\"dahlan\" + -0.205*\"arb\" + 0.188*\"sosok\" + 0.187*\"gubernur\" + 0.186*\"populer\" + 0.185*\"bincang\" + 0.169*\"tokoh\" + 0.168*\"nyata\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lsa_model[corpus_tfidf[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lsa_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8833066821098328\t \n",
      "Topic: 0.112*\"hatta\" + 0.052*\"ketua\" + 0.051*\"cawapres\" + 0.044*\"pan\" + 0.044*\"buka\" + 0.039*\"evaluasi\" + 0.039*\"pencapresan\" + 0.038*\"radjasa\" + 0.026*\"survey\" + 0.020*\"tweet\"\n",
      "\n",
      "Score: 0.06438399851322174\t \n",
      "Topic: 0.071*\"jk\" + 0.045*\"gubernur\" + 0.044*\"calon\" + 0.031*\"tokoh\" + 0.031*\"nyata\" + 0.030*\"mahfud\" + 0.028*\"indonesia\" + 0.027*\"pdip\" + 0.026*\"sosok\" + 0.025*\"populer\"\n",
      "\n",
      "Score: 0.052309323102235794\t \n",
      "Topic: 0.078*\"dahlan\" + 0.061*\"arb\" + 0.046*\"pilih\" + 0.038*\"iskan\" + 0.035*\"rakyat\" + 0.030*\"wiranto\" + 0.028*\"dukung\" + 0.027*\"jk\" + 0.024*\"ya\" + 0.020*\"konvensi\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4878930151462555\t \n",
      "Topic: 0.046*\"hatta\" + 0.038*\"calon\" + 0.037*\"indonesia\" + 0.033*\"pilih\" + 0.026*\"wapres\" + 0.026*\"jakarta\" + 0.025*\"ketua\" + 0.023*\"ya\" + 0.022*\"buka\" + 0.021*\"tweet\"\n",
      "\n",
      "Score: 0.4027964174747467\t \n",
      "Topic: 0.051*\"dahlan\" + 0.043*\"arb\" + 0.033*\"rakyat\" + 0.030*\"nyata\" + 0.029*\"iskan\" + 0.028*\"gubernur\" + 0.026*\"sby\" + 0.026*\"sosok\" + 0.025*\"tokoh\" + 0.025*\"maju\"\n",
      "\n",
      "Score: 0.10931061953306198\t \n",
      "Topic: 0.068*\"jk\" + 0.041*\"dukung\" + 0.038*\"wiranto\" + 0.032*\"mahfud\" + 0.032*\"pdip\" + 0.026*\"mega\" + 0.022*\"md\" + 0.022*\"kalah\" + 0.020*\"ical\" + 0.020*\"menang\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[corpus_tfidf[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8642910718917847\t \n",
      "Topic: 0.068*\"jk\" + 0.041*\"dukung\" + 0.038*\"wiranto\" + 0.032*\"mahfud\" + 0.032*\"pdip\" + 0.026*\"mega\" + 0.022*\"md\" + 0.022*\"kalah\" + 0.020*\"ical\" + 0.020*\"menang\"\n",
      "\n",
      "Score: 0.06800001859664917\t \n",
      "Topic: 0.051*\"dahlan\" + 0.043*\"arb\" + 0.033*\"rakyat\" + 0.030*\"nyata\" + 0.029*\"iskan\" + 0.028*\"gubernur\" + 0.026*\"sby\" + 0.026*\"sosok\" + 0.025*\"tokoh\" + 0.025*\"maju\"\n",
      "\n",
      "Score: 0.06770885735750198\t \n",
      "Topic: 0.046*\"hatta\" + 0.038*\"calon\" + 0.037*\"indonesia\" + 0.033*\"pilih\" + 0.026*\"wapres\" + 0.026*\"jakarta\" + 0.025*\"ketua\" + 0.023*\"ya\" + 0.022*\"buka\" + 0.021*\"tweet\"\n"
     ]
    }
   ],
   "source": [
    "fake_tweet = \"Saya mendukung JK dan Kalla! PDI-P selamanya!\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(fake_tweet))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
